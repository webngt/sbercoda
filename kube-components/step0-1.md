Давайте посмотрим на инфраструктурный слой кластера Kubernetes и работу его компонентов. 

Сначала создадим и развернем кластер **Kubernetes**. Для этого нужно запустить команду: 

`./launch_k8s.sh`{{execute}} 

и дождаться ее выполнения. В результате выполнения будет развернут полноценный кластер **Kubernetes** с двумя нодами: одной управляющей ноды и одной рабочей. Управляющая нода имеет имя хоста **controlplane**, а рабочая - **node01**. 

Зайдем на рабочую ноду в соседнем терминале.   

`ssh node01 `{{execute T2}}. 

> В силу особенностей работы Катакоды на команду надо будет нажать 2 раза - первый раз будет открыт терминал, а во второй выполнится уже сама команда.

## Агенты Kubernetes.

На всех нодах установлено контейнерное окружение - **Docker**. А также запущены агенты - **kubelet** и **kube-proxy**.

Убедиться в этом мы можем, запуском команды `ps -ef | grep /usr/bin/kubelet`, которая выведет все процессы, а потом отфильтрует те, которые содержат в команде запуска `/usr/bin/kubelet`.

Выполним команду на управляющей ноде.
`ps -ef | grep /usr/bin/kubelet`{{execute T1}}

и на рабочей: 

`ps -ef | grep /usr/bin/kubelet`{{execute T2}}

И таким образом убедимся, что **kubelet** запущен на всех нодах кластера.

Агент **Kube-proxy** запщуен в контейнерном окружении. 

Убедится, что он запущен можно с помощью команды `docker ps | grep -v pause |grep kube-proxy`. 

> с помощью команды `grep -v pause` из списка удаляются так называемые pause контейнеры, которые Kubernetes создает для своих технических нужд 

Выполним команду на управляющией ноде:

`docker ps | grep -v pause | grep kube-proxy`{{execute T1}}

и на рабочей: 

`docker ps | grep -v pause | grep kube-proxy`{{execute T2}}

и убедимся, что **Kube-proxy** запущен на всех нодах кластера. 

## Управляющие компоненты

Управляющие компоненты запущены только на **управляющией** ноде в контейнерном окружении. 

Очистим терминал и запустим команду на управляющей ноде, которая отфильтрует контейнеры с управлюящими компонентами  **etcd**, **kube-scheduler**, **kube-controller-manager** и **API Server**:

`clear`{{execute T1}}
`docker ps | grep -v pause | grep -E 'etcd|apiserver|scheduler|kube-controller-manager'`{{execute T1}}

И запустим ту же команду на рабочей ноде:

`clear`{{execute T2}}
`docker ps | grep -v pause | grep -E 'etcd|apiserver|scheduler|kube-controller-manager'`{{execute T2}}

Видим,  что управлящие компоненты запущены только на управляющей ноде.

Поскольку эта инсталляция **Kubernetes** не является минимальной, помимо базовых компонентов и агентов, тут присутствую дополнительные компоненты:

`clear`{{execute T1}}
`docker ps | grep -v pause`{{execute T1}}

В зрелых продакшн кластерах Kubernetes управляющий слой не ограничивается только тем минимальным набором компонентов, которые мы рассматриваем. Например, можно заметить сетевой плагин **flannel**, компонент работы с ДНС - **coredns** и т.д. 

`clear`{{execute T1}} `clear`{{execute T2}}
